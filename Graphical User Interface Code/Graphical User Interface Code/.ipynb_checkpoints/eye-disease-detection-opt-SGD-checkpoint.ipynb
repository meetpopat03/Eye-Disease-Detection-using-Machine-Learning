{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKeVGxZ5GG6o"
   },
   "source": [
    "# Import needed modules\n",
    "\n",
    "!pip install tensorflow==2.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CeMcAy_5GG6s",
    "outputId": "8e007371-6c2c-492c-99bb-172286922ae2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_12108\\60502905.py:11: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA_gwvwnGG6v"
   },
   "source": [
    "# Create needed functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4reLHLHabWD"
   },
   "source": [
    "## Functions to Create Data Frame from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQdhl_CRGG6v"
   },
   "source": [
    "#### **Function to create data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2nDmYaAabWE"
   },
   "outputs": [],
   "source": [
    "# Generate data paths with labels\n",
    "def define_paths(data_dir):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    folds = os.listdir(data_dir)\n",
    "    for fold in folds:\n",
    "        foldpath = os.path.join(data_dir, fold)\n",
    "        filelist = os.listdir(foldpath)\n",
    "        for file in filelist:\n",
    "            fpath = os.path.join(foldpath, file)\n",
    "            filepaths.append(fpath)\n",
    "            labels.append(fold)\n",
    "\n",
    "    return filepaths, labels\n",
    "\n",
    "\n",
    "# Concatenate data paths with labels into one dataframe ( to later be fitted into the model )\n",
    "def define_df(files, classes):\n",
    "    Fseries = pd.Series(files, name= 'filepaths')\n",
    "    Lseries = pd.Series(classes, name='labels')\n",
    "    return pd.concat([Fseries, Lseries], axis= 1)\n",
    "\n",
    "# Split dataframe to train, valid, and test\n",
    "def split_data(data_dir):\n",
    "    # train dataframe\n",
    "    files, classes = define_paths(data_dir)\n",
    "    df = define_df(files, classes)\n",
    "    strat = df['labels']\n",
    "    train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123, stratify= strat)\n",
    "\n",
    "    # valid and test dataframe\n",
    "    strat = dummy_df['labels']\n",
    "    valid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123, stratify= strat)\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZaHdeFxGG6x"
   },
   "source": [
    "#### Function to generate images from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLL8hHQcGG6x"
   },
   "outputs": [],
   "source": [
    "def create_gens (train_df, valid_df, test_df, batch_size):\n",
    "    '''\n",
    "    This function takes train, validation, and test dataframe and fit them into image data generator, because model takes data from image data generator.\n",
    "    Image data generator converts images into tensors. '''\n",
    "\n",
    "\n",
    "    # define model parameters\n",
    "    img_size = (224, 224)\n",
    "    channels = 3 # either BGR or Grayscale\n",
    "    color = 'rgb'\n",
    "    img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "    # Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
    "    ts_length = len(test_df)\n",
    "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "    test_steps = ts_length // test_batch_size\n",
    "\n",
    "    # This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
    "    def scalar(img):\n",
    "        return img\n",
    "\n",
    "    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)\n",
    "    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
    "\n",
    "    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "\n",
    "    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "\n",
    "    # Note: we will use custom test_batch_size, and make shuffle= false\n",
    "    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= False, batch_size= test_batch_size)\n",
    "\n",
    "    return train_gen, valid_gen, test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ifXox4SGG6y"
   },
   "source": [
    "#### **Function to display data sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAGbj3ZyGG6y"
   },
   "outputs": [],
   "source": [
    "def show_images(gen):\n",
    "    '''\n",
    "    This function take the data generator and show sample of the images\n",
    "    '''\n",
    "\n",
    "    # return classes , images to be displayed\n",
    "    g_dict = gen.class_indices        # defines dictionary {'class': index}\n",
    "    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes), classes names : string\n",
    "    images, labels = next(gen)        # get a batch size samples from the generator\n",
    "\n",
    "    # calculate number of displayed samples\n",
    "    length = len(labels)        # length of batch size\n",
    "    sample = min(length, 25)    # check if sample less than 25 images\n",
    "\n",
    "    plt.figure(figsize= (20, 20))\n",
    "\n",
    "    for i in range(sample):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = images[i] / 255       # scales data to range (0 - 255)\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])  # get image index\n",
    "        class_name = classes[index]   # get class of image\n",
    "        plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K-ryg0DGG6z"
   },
   "source": [
    "#### **Callbacks** \n",
    "<br> \n",
    "Callbacks : Helpful functions to help optimize model training  <br> \n",
    "Examples: stop model training after specfic time, stop training if no improve in accuracy and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5HiN8XDGG60"
   },
   "outputs": [],
   "source": [
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self.model = model\n",
    "        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n",
    "        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
    "        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
    "        self.factor = factor # factor by which to reduce the learning rate\n",
    "        self.batches = batches # number of training batch to run per epoch\n",
    "        self.epochs = epochs\n",
    "        self.ask_epoch = ask_epoch\n",
    "        self.ask_epoch_initial = ask_epoch # save this value to restore if restarting training\n",
    "\n",
    "        # callback variables\n",
    "        self.count = 0 # how many times lr has been reduced without improvement\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1   # epoch with the lowest loss\n",
    "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initial learning rate and save it\n",
    "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
    "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
    "        self.best_weights = self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.initial_weights = self.model.get_weights()   # save initial weights if they have to get restored\n",
    "\n",
    "    # Define a function that will run when train begins\n",
    "    def on_train_begin(self, logs= None):\n",
    "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
    "        print(msg)\n",
    "        ans = input('')\n",
    "        if ans in ['Y', 'y']:\n",
    "            self.ask_permission = 1\n",
    "        elif ans in ['N', 'n']:\n",
    "            self.ask_permission = 0\n",
    "\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs= None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print(msg)\n",
    "\n",
    "        # set the weights of the model to the best weights\n",
    "        self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs= None):\n",
    "        # get batch accuracy and loss\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "\n",
    "        # prints over on the same line to show running batch count\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end= '')\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs= None):\n",
    "        self.ep_start = time.time()\n",
    "\n",
    "\n",
    "    # Define method runs on the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs= None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "        current_lr = lr\n",
    "        acc = logs.get('accuracy')  # get training accuracy\n",
    "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
    "        loss = logs.get('loss')  # get training loss for this epoch\n",
    "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
    "\n",
    "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
    "            monitor = 'accuracy'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
    "\n",
    "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
    "                self.highest_tracc = acc # set new highest training accuracy\n",
    "                self.best_weights = self.model.get_weights() # training accuracy improved so save the weights\n",
    "                self.count = 0 # set count to 0 since training accuracy improved\n",
    "                self.stop_count = 0 # set stop counter to 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
    "\n",
    "            else:\n",
    "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
    "                # if so adjust learning rate\n",
    "                if self.count >= self.patience - 1: # lr should be adjusted\n",
    "                    lr = lr * self.factor # adjust the learning by factor\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
    "                    self.count = 0 # reset the count to 0\n",
    "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
    "                    self.count = 0 # reset counter\n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment patience counter\n",
    "\n",
    "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
    "            monitor = 'val_loss'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "\n",
    "            else:\n",
    "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
    "\n",
    "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
    "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
    "                self.best_weights = self.model.get_weights() # validation loss improved so save the weights\n",
    "                self.count = 0 # reset count since validation loss improved\n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
    "\n",
    "            else: # validation loss did not improve\n",
    "                if self.count >= self.patience - 1: # need to adjust lr\n",
    "                    lr = lr * self.factor # adjust the learning rate\n",
    "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
    "                    self.count = 0 # reset counter\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
    "\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment the patience counter\n",
    "\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
    "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)\n",
    "            self.model.stop_training = True # stop training\n",
    "\n",
    "        else:\n",
    "            if self.ask_epoch != None and self.ask_permission != 0:\n",
    "                if epoch + 1 >= self.ask_epoch:\n",
    "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
    "                    print(msg)\n",
    "\n",
    "                    ans = input('')\n",
    "                    if ans == 'H' or ans == 'h':\n",
    "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
    "                        print(msg)\n",
    "                        self.model.stop_training = True # stop training\n",
    "\n",
    "                    else:\n",
    "                        try:\n",
    "                            ans = int(ans)\n",
    "                            self.ask_epoch += ans\n",
    "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
    "                            print(msg)\n",
    "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
    "                            print(msg)\n",
    "\n",
    "                        except Exception:\n",
    "                            print('Invalid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zwhoj3zGG61"
   },
   "source": [
    "#### **Function to plot history of training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pU3eAW5jGG62"
   },
   "outputs": [],
   "source": [
    "def plot_training(hist):\n",
    "    '''\n",
    "    This function take training model and plot history of accuracy and losses with the best epoch in both of them.\n",
    "    '''\n",
    "\n",
    "    # Define needed variables\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize= (20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK6cgu7LGG63"
   },
   "source": [
    "#### **Function to create Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4mPYHnzGG64"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n",
    "\t'''\n",
    "\tThis function plot confusion matrix method from sklearn package.\n",
    "\t'''\n",
    "\n",
    "\tplt.figure(figsize= (10, 10))\n",
    "\tplt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
    "\tplt.title(title)\n",
    "\tplt.colorbar()\n",
    "\n",
    "\ttick_marks = np.arange(len(classes))\n",
    "\tplt.xticks(tick_marks, classes, rotation= 45)\n",
    "\tplt.yticks(tick_marks, classes)\n",
    "\n",
    "\tif normalize:\n",
    "\t\tcm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
    "\t\tprint('Normalized Confusion Matrix')\n",
    "\n",
    "\telse:\n",
    "\t\tprint('Confusion Matrix, Without Normalization')\n",
    "\n",
    "\tprint(cm)\n",
    "\n",
    "\tthresh = cm.max() / 2.\n",
    "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "\t\tplt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.ylabel('True Label')\n",
    "\tplt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57eDFl3oGG65"
   },
   "source": [
    "# **Model Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GHNMVrhGG65"
   },
   "source": [
    "#### **Start Reading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWfxfQEVabWS",
    "outputId": "d8be6a8d-5b19-49f7-bfa1-09a96ff58286"
   },
   "outputs": [],
   "source": [
    "data_dir = '/Virtual_Desktop/Desktop/Tech_Work/potato_disease_ml_project/dataset/dataset/'\n",
    "\n",
    "try:\n",
    "    # Get splitted data\n",
    "    train_df, valid_df, test_df = split_data(data_dir)\n",
    "\n",
    "    # Get Generators\n",
    "    batch_size = 40\n",
    "    train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n",
    "\n",
    "except:\n",
    "    print('Invalid Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Display Image Sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wvOKjeRGG65"
   },
   "source": [
    "#### **Generic Model Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDT4CV15abWT",
    "outputId": "365637a8-7535-4ac4-90ea-700f6eb5769e"
   },
   "outputs": [],
   "source": [
    "# Create Model Structure\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
    "\n",
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "\n",
    "# from tensorflow.keras.applications.efficientnet import EfficientNetB3\n",
    "model = Sequential([\n",
    "    # Conv2D layer with 64 filters and a 3x3 kernel\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=img_shape),\n",
    "    \n",
    "    # MaxPooling2D layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Batch Normalization layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Conv2D layer with 64 filters and a 3x3 kernel\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # MaxPooling2D layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Batch Normalization layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Conv2D layer with 128 filters and a 3x3 kernel\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    \n",
    "    # MaxPooling2D layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Batch Normalization layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Conv2D layer with 128 filters and a 3x3 kernel\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    \n",
    "    # MaxPooling2D layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Batch Normalization layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Conv2D layer with 128 filters and a 3x3 kernel\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    \n",
    "    # MaxPooling2D layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Batch Normalization layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Conv2D layer with 64 filters and a 3x3 kernel\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # MaxPooling2D layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Batch Normalization layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "    \n",
    "    # Dense layer with 256 units\n",
    "    Dense(256, activation='relu'),\n",
    "    \n",
    "    # Dense layer with 64 units\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # Dense layer with 4 units (assuming 4 classes for the output layer)\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(SGD(learning_rate= 0.01), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TciwhdM1GG66"
   },
   "source": [
    "#### **Set Callback Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7abvdv7mGG66"
   },
   "outputs": [],
   "source": [
    "batch_size = 40   # set batch size for training\n",
    "epochs = 2   # number of all epochs in training\n",
    "patience = 1   #number of epochs to wait to adjust lr if monitored value does not improve\n",
    "stop_patience = 3   # number of epochs to wait before stopping training if monitored value does not improve\n",
    "threshold = 0.9   # if train accuracy is < threshold adjust monitor accuracy, else monitor validation loss\n",
    "factor = 0.5   # factor to reduce lr by\n",
    "ask_epoch = 5   # number of epochs to run before asking if you want to halt training\n",
    "batches = int(np.ceil(len(train_gen.labels) / batch_size))    # number of training batch to run per epoch\n",
    "\n",
    "callbacks = [MyCallback(model= model, patience= patience, stop_patience= stop_patience, threshold= threshold,\n",
    "            factor= factor, batches= batches, epochs= epochs, ask_epoch= ask_epoch )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap89fjdxGG67"
   },
   "source": [
    "#### **Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Uk3BTERGG67",
    "outputId": "ec610f68-a1a5-4c7d-9969-26dfab2d0305"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n",
    "                    validation_data= valid_gen, validation_steps= None, shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNKq6ebOGG67"
   },
   "source": [
    "#### **Display model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0Bj0Sp_GG68",
    "outputId": "663963ec-ea21-4272-8dda-a16c5f5e2ce5"
   },
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MySXhfAJGG68"
   },
   "source": [
    "# **Evaluate model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSKDkyXXGG68",
    "outputId": "b521980b-a33b-421b-8cdf-4d92fb0f304a"
   },
   "outputs": [],
   "source": [
    "ts_length = len(test_df)\n",
    "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Validation Loss: \", valid_score[0])\n",
    "print(\"Validation Accuracy: \", valid_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l-DABtFGG68"
   },
   "source": [
    "# **Get Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDFj7MZdGG69",
    "outputId": "6dbce8ed-fc8c-4398-b8bd-1ce8cb403727"
   },
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_gen)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJscUTF6GG69"
   },
   "source": [
    "#### **Confusion Matrics and Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQR-UlD6GG69",
    "outputId": "09ac1d97-2053-4633-e066-ca11540a2e27"
   },
   "outputs": [],
   "source": [
    "g_dict = test_gen.class_indices\n",
    "classes = list(g_dict.keys())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_gen.classes, y_pred)\n",
    "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix')\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(test_gen.classes, y_pred, target_names= classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsIK5v0lGG69"
   },
   "source": [
    "#### **Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy5ShUciGG6-",
    "outputId": "6122a45f-351d-4cb4-f046-d141ab2f9a5e"
   },
   "outputs": [],
   "source": [
    "model_name = model.input_names[0][:-6]\n",
    "subject = 'Eye Disease'\n",
    "acc = test_score[1] * 100\n",
    "save_path = ''\n",
    "\n",
    "# Save model\n",
    "save_id = str(f'{model_name}-{subject}-{\"%.2f\" %round(acc, 2)}.h5')\n",
    "model_save_loc = os.path.join(save_path, save_id)\n",
    "model.save(model_save_loc)\n",
    "model_version=11\n",
    "model.save(f\"../models/{model_version}\")\n",
    "print(f'model was saved as {model_save_loc}')\n",
    "\n",
    "# Save weights\n",
    "weight_save_id = str(f'{model_name}-{subject}-weights.h5')\n",
    "weights_save_loc = os.path.join(save_path, weight_save_id)\n",
    "model.save_weights(weights_save_loc)\n",
    "print(f'weights were saved as {weights_save_loc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2fsiEtEGG6-"
   },
   "source": [
    "#### **Generate CSV files containing classes indicies & image size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiHQzq8XGG6-",
    "outputId": "e2daeab5-c65c-495c-ffde-be259c917c07"
   },
   "outputs": [],
   "source": [
    "class_dict = train_gen.class_indices\n",
    "img_size = train_gen.image_shape\n",
    "height = []\n",
    "width = []\n",
    "for _ in range(len(class_dict)):\n",
    "    height.append(img_size[0])\n",
    "    width.append(img_size[1])\n",
    "\n",
    "Index_series = pd.Series(list(class_dict.values()), name= 'class_index')\n",
    "Class_series = pd.Series(list(class_dict.keys()), name= 'class')\n",
    "Height_series = pd.Series(height, name= 'height')\n",
    "Width_series = pd.Series(width, name= 'width')\n",
    "class_df = pd.concat([Index_series, Class_series, Height_series, Width_series], axis= 1)\n",
    "csv_name = f'{subject}-class_dict.csv'\n",
    "csv_save_loc = os.path.join(save_path, csv_name)\n",
    "class_df.to_csv(csv_save_loc, index= False)\n",
    "print(f'class csv file was saved as {csv_save_loc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2440665,
     "sourceId": 4130910,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 2659,
     "sourceId": 3732,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30407,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "newkernel",
   "language": "python",
   "name": "newkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
